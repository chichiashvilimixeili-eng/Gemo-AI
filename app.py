import streamlit as st
from huggingface_hub import InferenceClient

# --- áƒáƒ®áƒáƒšáƒ˜ "áƒ¢áƒ•áƒ˜áƒœáƒ˜áƒ¡" áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ ---
# áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ— Mistral-áƒ¡, áƒ áƒáƒ›áƒ”áƒšáƒ˜áƒª áƒ¡áƒ¢áƒáƒ‘áƒ˜áƒšáƒ£áƒ áƒ˜áƒ áƒ“áƒ áƒ§áƒáƒ•áƒ”áƒšáƒ—áƒ•áƒ˜áƒ¡ áƒáƒáƒ¡áƒ£áƒ®áƒáƒ‘áƒ¡
client = InferenceClient(api_key="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") # áƒ“áƒ áƒáƒ”áƒ‘áƒ˜áƒ—áƒ˜ áƒ’áƒáƒ¡áƒáƒ¦áƒ”áƒ‘áƒ˜

st.set_page_config(page_title="Gemo AI v2", page_icon="ğŸš€")
st.title("ğŸš€ Gemo AI: áƒáƒ®áƒáƒšáƒ˜ áƒ”áƒ áƒ")
st.info("Gemo áƒáƒ®áƒšáƒ Hugging Face-áƒ˜áƒ¡ áƒ«áƒ áƒáƒ•áƒ–áƒ” áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡!")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("áƒ°áƒ™áƒ˜áƒ—áƒ®áƒ” áƒ áƒáƒ›áƒ” Gemo-áƒ¡..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        try:
            # Gemo áƒáƒ’áƒ”áƒœáƒ”áƒ áƒ˜áƒ áƒ”áƒ‘áƒ¡ áƒáƒáƒ¡áƒ£áƒ®áƒ¡
            response = ""
            for message in client.chat_completion(
                model="mistralai/Mistral-7B-Instruct-v0.3",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                stream=True,
            ):
                token = message.choices[0].delta.content
                response += token

            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
        except Exception as e:
            st.error(f"áƒáƒáƒ”áƒ áƒáƒªáƒ˜áƒ áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ¡áƒ áƒ£áƒšáƒ“áƒ: {e}")
